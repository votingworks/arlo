import pytest
import math
import numpy as np

from sampler import Sampler


@pytest.fixture
def sampler():
    seed = "12345678901234567890abcdefghijklmnopqrstuvwxyzðŸ˜Š"

    risk_limit = 0.1
    contests = {
        "test1": {"cand1": 600, "cand2": 400, "ballots": 1000, "numWinners": 1},
        "test2": {
            "cand1": 600,
            "cand2": 200,
            "cand3": 100,
            "ballots": 900,
            "numWinners": 1,
        },
        "test3": {"cand1": 100, "ballots": 100, "numWinners": 1},
        "test4": {"cand1": 100, "ballots": 100, "numWinners": 1},
        "test5": {"cand1": 500, "cand2": 500, "ballots": 1000, "numWinners": 1},
        "test6": {
            "cand1": 300,
            "cand2": 200,
            "cand3": 200,
            "ballots": 1000,
            "numWinners": 1,
        },
        "test7": {
            "cand1": 300,
            "cand2": 200,
            "cand3": 100,
            "ballots": 700,
            "numWinners": 2,
        },
        "test7": {
            "cand1": 300,
            "cand2": 200,
            "cand3": 100,
            "ballots": 700,
            "numWinners": 2,
        },
        "test8": {
            "cand1": 300,
            "cand2": 300,
            "cand3": 100,
            "ballots": 700,
            "numWinners": 2,
        },
        "test9": {"cand1": 300, "cand2": 200, "ballots": 700, "numWinners": 2},
        "test10": {
            "cand1": 600,
            "cand2": 300,
            "cand3": 100,
            "ballots": 1000,
            "numWinners": 2,
        },
    }

    yield Sampler("BRAVO", seed, risk_limit, contests)


def test_expected_sample_sizes(sampler):
    # Test expected sample sizes computation

    true_asns = {
        "test1": 119,
        "test2": 22,
        "test3": -1,
        "test4": -1,
        "test5": 1000,
        "test6": 238,
        "test7": 101,
        "test8": 34,
        "test9": -1,
        "test10": 48,
    }

    computed_asns = sampler.audit.get_expected_sample_sizes(
        sampler.margins, sampler.contests, round0_sample_results
    )
    for contest in true_asns:
        expected = true_asns[contest]
        computed = computed_asns[contest]

        assert (
            expected == computed
        ), "get_expected_sample_sizes failed in {}: got {}, expected {}".format(
            contest, computed, expected
        )


def test_expected_sample_sizes_second_round(sampler):
    # Test expected sample sizes computation

    true_asns = {
        "test1": -12,
        "test2": 42,
        "test3": -1,
        "test4": -1,
        "test5": 1000,
        "test6": -2,
        "test7": -28,
        "test8": 14,
        "test9": -1,
        "test10": -52,
    }

    computed_asns = sampler.audit.get_expected_sample_sizes(
        sampler.margins, sampler.contests, round1_sample_results
    )
    for contest in true_asns:
        expected = true_asns[contest]
        computed = computed_asns[contest]

        assert (
            expected == computed
        ), "get_expected_sample_sizes failed in {}: got {}, expected {}".format(
            contest, computed, expected
        )


def test_bravo_sample_sizes(sampler):
    # Test bravo sample simulator
    # Test without sample
    expected_size1 = 1599
    r0_sample_win = 0
    r0_sample_rup = 0

    computed_size1 = math.ceil(
        sampler.audit.bravo_sample_sizes(
            p_w=0.4,
            p_r=0.32,
            sample_w=r0_sample_win,
            sample_r=r0_sample_rup,
            p_completion=0.9,
        )
    )
    delta = expected_size1 - computed_size1

    assert not delta, "bravo_sample_sizes failed: got {}, expected {}".format(
        computed_size1, expected_size1
    )

    expected_size1 = 6067

    computed_size1 = math.ceil(
        sampler.audit.bravo_sample_sizes(
            p_w=0.36,
            p_r=0.32,
            sample_w=r0_sample_win,
            sample_r=r0_sample_rup,
            p_completion=0.9,
        )
    )
    delta = expected_size1 - computed_size1

    assert not delta, "bravo_sample_sizes failed: got {}, expected {}".format(
        computed_size1, expected_size1
    )

    expected_size1 = 2475

    computed_size1 = math.ceil(
        sampler.audit.bravo_sample_sizes(
            p_w=0.36,
            p_r=0.32,
            sample_w=r0_sample_win,
            sample_r=r0_sample_rup,
            p_completion=0.6,
        )
    )
    delta = expected_size1 - computed_size1

    assert not delta, "bravo_sample_sizes failed: got {}, expected {}".format(
        computed_size1, expected_size1
    )

    expected_size1 = 5657

    computed_size1 = math.ceil(
        sampler.audit.bravo_sample_sizes(
            p_w=0.52,
            p_r=0.47,
            sample_w=r0_sample_win,
            sample_r=r0_sample_rup,
            p_completion=0.9,
        )
    )
    delta = expected_size1 - computed_size1

    assert not delta, "bravo_sample_sizes failed: got {}, expected {}".format(
        computed_size1, expected_size1
    )


def test_bravo_sample_sizes_round1_finish(sampler):
    # Guarantee that the audit should have finished
    r0_sample_win = 10000
    r0_sample_rup = 0
    expected_size1 = 0

    computed_size1 = math.ceil(
        sampler.audit.bravo_sample_sizes(
            p_w=0.52,
            p_r=0.47,
            sample_w=r0_sample_win,
            sample_r=r0_sample_rup,
            p_completion=0.9,
        )
    )
    delta = expected_size1 - computed_size1

    assert not delta, "bravo_sample_sizes failed: got {}, expected {}".format(
        computed_size1, expected_size1
    )


def test_bravo_sample_sizes_round1_incomplete(sampler):
    expected_size1 = 2636
    r0_sample_win = 2923
    r0_sample_rup = 2735

    computed_size1 = math.ceil(
        sampler.audit.bravo_sample_sizes(
            p_w=0.52,
            p_r=0.47,
            sample_w=r0_sample_win,
            sample_r=r0_sample_rup,
            p_completion=0.9,
        )
    )
    delta = expected_size1 - computed_size1

    assert not delta, "bravo_sample_sizes failed: got {}, expected {}".format(
        computed_size1, expected_size1
    )


def test_bravo_expected_prob(sampler):
    # Test bravo sample simulator
    # Test without sample
    expected_prob1 = 0.52
    r0_sample_win = round0_sample_results["test1"]["cand1"]
    r0_sample_rup = round0_sample_results["test1"]["cand2"]

    computed_prob1 = round(
        sampler.audit.expected_prob(
            p_w=0.6, p_r=0.4, sample_w=r0_sample_win, sample_r=r0_sample_rup, asn=119
        ),
        2,
    )
    delta = expected_prob1 - computed_prob1

    assert not delta, "bravo_simulator failed: got {}, expected {}".format(
        computed_prob1, expected_prob1
    )


def test_bravo_sample_sizes(sampler):
    # Test retrieving menu of sample sizes
    computed_samples = sampler.get_sample_sizes(round0_sample_results)
    print(computed_samples)
    for contest in computed_samples:
        for key in true_sample_sizes[contest]:
            if key == "asn":
                # Check probs:
                if sampler.contests[contest]["numWinners"] == 1:
                    expected_prob = true_sample_sizes[contest][key]["prob"]
                    computed_prob = round(computed_samples[contest][key]["prob"], 2)

                    assert (
                        expected_prob == computed_prob
                    ), "{} expected_sample_size probabability check for {} failed: got {}, expected {}".format(
                        key, contest, computed_prob, expected_prob
                    )

                expected = true_sample_sizes[contest][key]["size"]
                computed = computed_samples[contest][key]["size"]
            else:
                expected = true_sample_sizes[contest][key]
                computed = computed_samples[contest][key]
            diff = expected - computed

            assert not diff, "{} sample size for {} failed: got {}, expected {}".format(
                key, contest, computed, expected
            )


def test_compute_margins(sampler):
    # Test margins
    true_margins = {
        "test1": {
            "winners": {"cand1": {"p_w": 0.6, "s_w": 0.6, "swl": {"cand2": 0.6}}},
            "losers": {"cand2": {"p_l": 0.4, "s_l": 0.4}},
        },
        "test2": {
            "winners": {
                "cand1": {
                    "p_w": 2 / 3,
                    "s_w": 2 / 3,
                    "swl": {"cand2": 6 / 8, "cand3": 6 / 7},
                }
            },
            "losers": {
                "cand2": {"p_l": 2 / 9, "s_l": 2 / 9},
                "cand3": {"p_l": 1 / 9, "s_l": 1 / 9},
            },
        },
        "test3": {"winners": {"cand1": {"p_w": 1, "s_w": 1, "swl": {}}}, "losers": {}},
        "test4": {"winners": {"cand1": {"p_w": 1, "s_w": 1, "swl": {}}}, "losers": {}},
        "test5": {
            "winners": {"cand1": {"p_w": 0.5, "s_w": 0.5, "swl": {"cand2": 0.5}}},
            "losers": {"cand2": {"p_l": 0.5, "s_l": 0.5}},
        },
        "test6": {
            "winners": {
                "cand1": {
                    "p_w": 0.3,
                    "s_w": 300 / 700,
                    "swl": {"cand2": 300 / (300 + 200), "cand3": 300 / (300 + 200)},
                }
            },
            "losers": {
                "cand2": {"p_l": 0.2, "s_l": 200 / 700},
                "cand3": {"p_l": 0.2, "s_l": 200 / 700},
            },
        },
        "test7": {
            "winners": {
                "cand1": {
                    "p_w": 300 / 700,
                    "s_w": 300 / 600,
                    "swl": {"cand3": 300 / (300 + 100)},
                },
                "cand2": {
                    "p_w": 200 / 700,
                    "s_w": 200 / 600,
                    "swl": {"cand3": 200 / (200 + 100)},
                },
            },
            "losers": {"cand3": {"p_l": 100 / 700, "s_l": 100 / 600}},
        },
        "test8": {
            "winners": {
                "cand1": {
                    "p_w": 300 / 700,
                    "s_w": 300 / 700,
                    "swl": {"cand3": 300 / (300 + 100)},
                },
                "cand2": {
                    "p_w": 300 / 700,
                    "s_w": 300 / 700,
                    "swl": {"cand3": 300 / (300 + 100)},
                },
            },
            "losers": {"cand3": {"p_l": 100 / 700, "s_l": 100 / 700}},
        },
        "test9": {
            "winners": {
                "cand1": {"p_w": 300 / 700, "s_w": 300 / 500, "swl": {}},
                "cand2": {"p_w": 200 / 700, "s_w": 200 / 500, "swl": {}},
            },
            "losers": {},
        },
        "test10": {
            "winners": {
                "cand1": {
                    "p_w": 600 / 1000,
                    "s_w": 600 / 1000,
                    "swl": {"cand3": 600 / 700},
                },
                "cand2": {
                    "p_w": 300 / 1000,
                    "s_w": 300 / 1000,
                    "swl": {"cand3": 300 / 400},
                },
            },
            "losers": {"cand3": {"p_l": 100 / 1000, "s_l": 100 / 1000}},
        },
    }

    margins = sampler.compute_margins()
    for contest in margins:
        true_margins_for_contest = true_margins[contest]
        computed_margins_for_contest = margins[contest]

        for winner in true_margins_for_contest["winners"]:
            expected = round(true_margins_for_contest["winners"][winner]["p_w"], 5)
            computed = round(computed_margins_for_contest["winners"][winner]["p_w"], 5)
            assert expected == computed, "{} p_w failed: got {}, expected {}".format(
                contest, computed, expected
            )

            expected = round(true_margins_for_contest["winners"][winner]["s_w"], 5)
            computed = round(computed_margins_for_contest["winners"][winner]["s_w"], 5)
            assert expected == computed, "{} s_w failed: got {}, expected {}".format(
                contest, computed, expected
            )

            for cand in true_margins_for_contest["winners"][winner]["swl"]:
                expected = round(
                    true_margins_for_contest["winners"][winner]["swl"][cand], 5
                )
                computed = round(
                    computed_margins_for_contest["winners"][winner]["swl"][cand], 5
                )
                assert (
                    expected == computed
                ), "{} swl failed: got {}, expected {}".format(
                    contest, computed, expected
                )

        for loser in true_margins_for_contest["losers"]:
            expected = round(true_margins_for_contest["losers"][loser]["p_l"], 5)
            computed = round(computed_margins_for_contest["losers"][loser]["p_l"], 5)
            assert expected == computed, "{} p_l failed: got {}, expected {}".format(
                contest, computed, expected
            )

            expected = round(true_margins_for_contest["losers"][loser]["s_l"], 5)
            computed = round(computed_margins_for_contest["losers"][loser]["s_l"], 5)
            assert expected == computed, "{} s_l failed: got {}, expected {}".format(
                contest, computed, expected
            )


def test_compute_risk(sampler):
    # Test computing sample
    expected_Ts = {
        "test1": {("cand1", "cand2"): 0.07},
        "test2": {("cand1", "cand2"): 10.38, ("cand1", "cand3"): 0,},
        "test3": {("cand1",): 1},
        "test4": {("cand1",): 1},
        "test5": {("cand1", "cand2"): 1},
        "test6": {("cand1", "cand2"): 0.08, ("cand1", "cand3"): 0.08,},
        "test7": {("cand1", "cand3"): 0.01, ("cand2", "cand3"): 0.04,},
        "test8": {("cand1", "cand3"): 0.0, ("cand2", "cand3"): 0.22,},
        "test9": {("cand1",): 1, ("cand2",): 1,},
        "test10": {("cand1", "cand3"): 0, ("cand2", "cand3"): 0.01,},
    }

    expected_decisions = {
        "test1": True,
        "test2": False,
        "test3": False,
        "test4": False,
        "test5": False,
        "test6": True,
        "test7": True,
        "test8": False,
        "test9": False,
        "test10": True,
    }

    for contest, sample in round1_sample_results.items():
        T, decision = sampler.compute_risk(contest, sample)
        expected_T = expected_Ts[contest]
        for pair in expected_T:
            diff = T[pair] - expected_T[pair]
            assert (
                abs(diff) < 0.01
            ), "Risk compute for {} failed! Expected {}, got {}".format(
                contest, expected_Ts[contest][pair], T[pair]
            )

        expected_decision = expected_decisions[contest]
        assert (
            decision == expected_decision
        ), "Risk decision for {} failed! Expected {}, got{}".format(
            contest, expected_decision, decision
        )


# Useful test data
round0_sample_results = {
    "test1": {"cand1": 0, "cand2": 0,},
    "test2": {"cand1": 0, "cand2": 0, "cand3": 0,},
    "test3": {"cand1": 0,},
    "test4": {"cand1": 0,},
    "test5": {"cand1": 0, "cand2": 0,},
    "test6": {"cand1": 0, "cand2": 0, "cand3": 0},
    "test7": {"cand1": 0, "cand2": 0, "cand3": 0},
    "test8": {"cand1": 0, "cand2": 0, "cand3": 0},
    "test9": {"cand1": 0, "cand2": 0, "cand3": 0},
    "test10": {"cand1": 0, "cand2": 0, "cand3": 0},
}

round1_sample_results = {
    "test1": {"cand1": 72, "cand2": 47},
    "test2": {"cand1": 25, "cand2": 18, "cand3": 5,},
    "test3": {"cand1": 0},
    "test4": {"cand1": 100},
    "test5": {"cand1": 500, "cand2": 500,},
    "test6": {"cand1": 72, "cand2": 48, "cand3": 48},
    "test7": {"cand1": 30, "cand2": 25, "cand3": 10},
    "test8": {"cand1": 72, "cand2": 55, "cand3": 30},
    "test9": {"cand1": 1, "cand2": 1,},
    "test10": {"cand1": 60, "cand2": 30, "cand3": 10},
}

true_sample_sizes = {
    "test1": {"asn": {"size": 119, "prob": 0.52}, 0.7: 184, 0.8: 244, 0.9: 351,},
    "test2": {"asn": {"size": 22, "prob": 0.6}, 0.7: 32, 0.8: 41, 0.9: 57,},
    "test3": {"asn": {"size": -1, "prob": -1}, 0.7: -1, 0.8: -1, 0.9: -1,},
    "test4": {"asn": {"size": -1, "prob": -1}, 0.7: -1, 0.8: -1, 0.9: -1,},
    "test5": {"asn": {"size": 1000, "prob": 1}, 0.7: 1000, 0.8: 1000, 0.9: 1000},
    "test6": {"asn": {"size": 238, "prob": 0.79}, 0.7: 368, 0.8: 488, 0.9: 702},
    "test7": {"asn": {"size": 101, "prob": None,},},
    "test8": {"asn": {"size": 59, "prob": None,},},
    "test9": {"asn": {"size": -1, "prob": None,},},
    "test10": {"asn": {"size": 48, "prob": None,},},
}
